{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e9d09b-c6a5-499b-a081-8071bb0e3cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (2025.3.0)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: datasets in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade fsspec datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585f75f7-8c6a-4d26-9569-83fb3f022684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efeac866-2e03-44f1-8b7a-9c58da2d6ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n",
      "\n",
      "2. Column Names (Features):\n",
      "['article', 'highlights', 'id']\n",
      "\n",
      "3. First Example from Training Split:\n",
      "Article (first 500 chars):\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s...\n",
      "\n",
      "Highlights:\n",
      "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
      "\n",
      "ID: 42c027e4ff9730fbb3de84c1af0d2c506e41c3e4\n",
      "\n",
      "4. Another Example (e.g., 5th example from validation split):\n",
      "Article (first 500 chars):\n",
      "(CNN)A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online. The parents of Cayman Naib, 13, have been communicating through the Facebook group \"Find Cayman\" since a day after his disappearance, according to close friend David Binswanger. Newtown Police say Cayman was last seen wearing a gray down winter jacket, black ski pants and hiking boots. He could be in the Radn...\n",
      "\n",
      "Highlights:\n",
      "Cayman Naib, 13, hasn't been heard from since Wednesday .\n",
      "Police, family, volunteers search for eighth-grader .\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (replace 'cnn_dailymail', '3.0.0' with your chosen dataset if different)\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "print(\"1. Dataset Structure:\")\n",
    "print(dataset) # Shows splits, number of rows, and features\n",
    "\n",
    "print(\"\\n2. Column Names (Features):\")\n",
    "print(dataset['train'].column_names) # Shows the names of columns (e.g., 'article', 'highlights', 'id')\n",
    "\n",
    "print(\"\\n3. First Example from Training Split:\")\n",
    "# Access the first example. Use slicing for long texts to avoid overwhelming output.\n",
    "first_example = dataset['train'][0]\n",
    "print(\"Article (first 500 chars):\")\n",
    "print(first_example['article'][:500] + \"...\")\n",
    "print(\"\\nHighlights:\")\n",
    "print(first_example['highlights'])\n",
    "print(\"\\nID:\", first_example['id'])\n",
    "\n",
    "print(\"\\n4. Another Example (e.g., 5th example from validation split):\")\n",
    "second_example = dataset['validation'][4] # Using index 4 for the 5th example\n",
    "print(\"Article (first 500 chars):\")\n",
    "print(second_example['article'][:500] + \"...\")\n",
    "print(\"\\nHighlights:\")\n",
    "print(second_example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d597c6f6-e6e8-4d47-8853-2f8677f23d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Removing Duplicates ---\n",
      "Original train size: 287113\n",
      "train size after deduplication: 284015\n",
      "Original validation size: 13368\n",
      "validation size after deduplication: 13368\n",
      "Original test size: 11490\n",
      "test size after deduplication: 11488\n",
      "\n",
      "Cleaned Dataset Structure (after deduplication):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id', '__index_level_0__'],\n",
      "        num_rows: 284015\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id', '__index_level_0__'],\n",
      "        num_rows: 11488\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming 'dataset' is already loaded from Step 1\n",
    "\n",
    "print(\"\\n--- Step 2: Removing Duplicates ---\")\n",
    "\n",
    "# Function to remove duplicates from a single split\n",
    "def remove_duplicates_from_split(split_name, data_split):\n",
    "    initial_size = len(data_split)\n",
    "    df = pd.DataFrame(data_split)\n",
    "    # Define columns that constitute a unique example. For summarization, it's typically article and highlights.\n",
    "    df.drop_duplicates(subset=['article', 'highlights'], inplace=True)\n",
    "    cleaned_split = Dataset.from_pandas(df)\n",
    "    print(f\"Original {split_name} size: {initial_size}\")\n",
    "    print(f\"{split_name} size after deduplication: {len(cleaned_split)}\")\n",
    "    return cleaned_split\n",
    "\n",
    "# Apply to each split\n",
    "cleaned_dataset = DatasetDict({\n",
    "    'train': remove_duplicates_from_split('train', dataset['train']),\n",
    "    'validation': remove_duplicates_from_split('validation', dataset['validation']),\n",
    "    'test': remove_duplicates_from_split('test', dataset['test'])\n",
    "})\n",
    "\n",
    "# Verify the structure remains the same\n",
    "print(\"\\nCleaned Dataset Structure (after deduplication):\")\n",
    "print(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f90a05a-febc-4574-a3ea-cb08e2a8ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Handling Missing/Empty Values ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████| 284015/284015 [00:01<00:00, 201662.89 examples/s]\n",
      "Filter: 100%|██████████████████| 13368/13368 [00:00<00:00, 227052.40 examples/s]\n",
      "Filter: 100%|██████████████████| 11488/11488 [00:00<00:00, 203854.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size after removing empty examples: 284015\n",
      "Validation size after removing empty examples: 13368\n",
      "Test size after removing empty examples: 11488\n",
      "\n",
      "Cleaned Dataset Structure (after empty examples removal):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id', '__index_level_0__'],\n",
      "        num_rows: 284015\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id', '__index_level_0__'],\n",
      "        num_rows: 11488\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'cleaned_dataset' is the result from Step 2\n",
    "\n",
    "print(\"\\n--- Step 3: Handling Missing/Empty Values ---\")\n",
    "\n",
    "def remove_empty_examples(example):\n",
    "    # Check if both 'article' and 'highlights' exist and are non-empty strings\n",
    "    return bool(example['article'] and example['highlights'] and\n",
    "                isinstance(example['article'], str) and isinstance(example['highlights'], str))\n",
    "\n",
    "# Apply this filter to all splits\n",
    "cleaned_dataset = cleaned_dataset.filter(remove_empty_examples)\n",
    "\n",
    "print(f\"Train size after removing empty examples: {len(cleaned_dataset['train'])}\")\n",
    "print(f\"Validation size after removing empty examples: {len(cleaned_dataset['validation'])}\")\n",
    "print(f\"Test size after removing empty examples: {len(cleaned_dataset['test'])}\")\n",
    "\n",
    "print(\"\\nCleaned Dataset Structure (after empty examples removal):\")\n",
    "print(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3e1c1b-96c7-4e0d-9591-570c3862ff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Basic Text Normalization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████| 284015/284015 [00:38<00:00, 7438.00 examples/s]\n",
      "Map: 100%|███████████████████████| 13368/13368 [00:01<00:00, 7698.40 examples/s]\n",
      "Map: 100%|███████████████████████| 11488/11488 [00:01<00:00, 7445.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from train split after normalization (first 500 chars):\n",
      "Article: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s...\n",
      "Highlights: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday . Young actor says he has no plans to fritter his cash away . Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Assuming 'cleaned_dataset' is the result from Step 3\n",
    "\n",
    "print(\"\\n--- Step 4: Basic Text Normalization ---\")\n",
    "\n",
    "def normalize_text_fields(examples):\n",
    "    # Process 'article'\n",
    "    normalized_articles = []\n",
    "    for text in examples['article']:\n",
    "        text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
    "        text = text.strip()              # Remove leading/trailing whitespace\n",
    "        normalized_articles.append(text)\n",
    "\n",
    "    # Process 'highlights'\n",
    "    normalized_highlights = []\n",
    "    for text in examples['highlights']:\n",
    "        text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
    "        text = text.strip()              # Remove leading/trailing whitespace\n",
    "        normalized_highlights.append(text)\n",
    "\n",
    "    return {'article': normalized_articles, 'highlights': normalized_highlights}\n",
    "\n",
    "# Apply this normalization to all splits\n",
    "cleaned_dataset = cleaned_dataset.map(normalize_text_fields, batched=True)\n",
    "\n",
    "print(f\"Example from train split after normalization (first 500 chars):\")\n",
    "print(\"Article:\", cleaned_dataset['train'][0]['article'][:500] + \"...\")\n",
    "print(\"Highlights:\", cleaned_dataset['train'][0]['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21089b09-c609-4a54-9174-b521acd53d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Tokenization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 284015/284015 [07:06<00:00, 665.49 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Column to remove ['__index_level_0__'] not in the dataset. Current columns in the dataset: ['article', 'highlights', 'id']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Apply the tokenization function to all splits of your dataset.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 'remove_columns' removes the original text columns to save memory, as they are no longer needed.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcleaned_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# num_proc=4, # Keep this commented out or set to 1 for now to avoid the multiprocessing error\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# num_proc=1, # Explicitly use a single process for debugging/stability\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# --- CRITICAL CHANGE HERE ---\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Explicitly list all columns to remove, including the pandas index column\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhighlights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__index_level_0__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTokenized Dataset Structure:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages/datasets/dataset_dict.py:944\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    942\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 944\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    966\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[0;32m~/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2999\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   2997\u001b[0m     missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(remove_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2999\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3000\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn to remove \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3001\u001b[0m         )\n\u001b[1;32m   3003\u001b[0m load_from_cache_file \u001b[38;5;241m=\u001b[39m load_from_cache_file \u001b[38;5;28;01mif\u001b[39;00m load_from_cache_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_caching_enabled()\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Column to remove ['__index_level_0__'] not in the dataset. Current columns in the dataset: ['article', 'highlights', 'id']"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict # Ensure DatasetDict is imported if you're using it\n",
    "\n",
    "# Assuming 'cleaned_dataset' is the result from Step 4\n",
    "\n",
    "print(\"\\n--- Step 5: Tokenization ---\")\n",
    "\n",
    "# Choose your pre-trained model checkpoint.\n",
    "# This choice dictates the tokenizer and the model architecture you'll use for fine-tuning.\n",
    "# For summarization, common choices are \"facebook/bart-large-cnn\", \"t5-base\", or \"google/pegasus-cnn_dailymail\"\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define maximum input and target lengths.\n",
    "# These are typically derived from the model's architecture limits and dataset statistics.\n",
    "# For BART/T5, max_input_length is often 1024. For CNN/DM highlights, 128-150 is usually sufficient.\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input articles\n",
    "    # truncation=True: cuts off text longer than max_input_length\n",
    "    # padding=\"max_length\": pads shorter texts to max_input_length with zeros\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"article\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Tokenize the target summaries (highlights)\n",
    "    # The 'labels' key is specifically expected by the Hugging Face Trainer for sequence-to-sequence models.\n",
    "    labels = tokenizer(\n",
    "        examples[\"highlights\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Assign the tokenized labels (input_ids) to the 'labels' key in model_inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization function to all splits of your dataset.\n",
    "# 'remove_columns' removes the original text columns to save memory, as they are no longer needed.\n",
    "tokenized_dataset = cleaned_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    # num_proc=4, # Keep this commented out or set to 1 for now to avoid the multiprocessing error\n",
    "    # num_proc=1, # Explicitly use a single process for debugging/stability\n",
    "    # --- CRITICAL CHANGE HERE ---\n",
    "    # Explicitly list all columns to remove, including the pandas index column\n",
    "    remove_columns=['article', 'highlights', 'id', '__index_level_0__']\n",
    ")\n",
    "\n",
    "print(\"\\nTokenized Dataset Structure:\")\n",
    "print(tokenized_dataset)\n",
    "print(\"\\nFirst Tokenized Example (training split - showing input_ids and labels):\")\n",
    "print(\"Article Input IDs (first 20):\", tokenized_dataset['train'][0]['input_ids'][:20])\n",
    "print(\"Article Attention Mask (first 20):\", tokenized_dataset['train'][0]['attention_mask'][:20])\n",
    "print(\"Highlights Labels (first 20):\", tokenized_dataset['train'][0]['labels'][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3bff85-9e2c-44d7-a4be-ec9521a27867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a00e2-f153-48c7-95b8-44e00bf95ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ffb9f0f-1a56-4688-9c0e-392cb82b3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CNN/DailyMail dataset (small samples for debugging)...\n",
      "Initial small dataset samples loaded successfully!\n",
      "Train split size: 1000\n",
      "Validation split size: 100\n",
      "Test split size: 100\n",
      "\n",
      "Starting deduplication process on small samples...\n",
      "Original train size: 1000\n",
      "train size after deduplication: 959\n",
      "Original validation size: 100\n",
      "validation size after deduplication: 100\n",
      "Original test size: 100\n",
      "test size after deduplication: 100\n",
      "\n",
      "Cleaned Dataset Structure (after deduplication):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 959\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Step 5: Tokenization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████| 959/959 [00:01<00:00, 759.15 examples/s]\n",
      "Map: 100%|████████████████████████████| 100/100 [00:00<00:00, 702.69 examples/s]\n",
      "Map: 100%|████████████████████████████| 100/100 [00:00<00:00, 698.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 959\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "\n",
      "First Tokenized Example (training split - showing input_ids and labels):\n",
      "Article Input IDs (first 20): [0, 574, 4524, 6, 1156, 36, 1251, 43, 480, 3268, 10997, 999, 3028, 7312, 20152, 3077, 899, 7, 10, 431]\n",
      "Article Attention Mask (first 20): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Highlights Labels (first 20): [0, 29345, 10997, 999, 3028, 7312, 20152, 1516, 984, 844, 448, 13016, 25, 37, 4072, 504, 302, 479, 50118, 22138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict # Ensure Dataset, DatasetDict are imported\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer # Added AutoTokenizer import here for clarity\n",
    "\n",
    "# --- Step 4: Load and Deduplicate Dataset ---\n",
    "# Load the CNN/DailyMail dataset (using small samples for debugging)\n",
    "print(\"Loading CNN/DailyMail dataset (small samples for debugging)...\")\n",
    "\n",
    "# Load a very small subset for faster debugging\n",
    "train_dataset_small = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1000]\")\n",
    "validation_dataset_small = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:100]\")\n",
    "test_dataset_small = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:100]\")\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "cleaned_dataset = DatasetDict({\n",
    "    'train': train_dataset_small,\n",
    "    'validation': validation_dataset_small,\n",
    "    'test': test_dataset_small\n",
    "})\n",
    "\n",
    "print(\"Initial small dataset samples loaded successfully!\")\n",
    "print(f\"Train split size: {len(cleaned_dataset['train'])}\")\n",
    "print(f\"Validation split size: {len(cleaned_dataset['validation'])}\")\n",
    "print(f\"Test split size: {len(cleaned_dataset['test'])}\")\n",
    "\n",
    "\n",
    "# Function to remove duplicates from a dataset split\n",
    "def remove_duplicates_from_split(split_name, split_dataset):\n",
    "    initial_size = len(split_dataset)\n",
    "    # Convert to pandas DataFrame for easy deduplication\n",
    "    df = split_dataset.to_pandas()\n",
    "    # Define columns that constitute a unique example. For summarization, it's typically article and highlights.\n",
    "    df.drop_duplicates(subset=['article', 'highlights'], inplace=True)\n",
    "    # CRITICAL CHANGE: Reset index and drop the old index column to prevent __index_level_0__\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Convert back to Hugging Face Dataset\n",
    "    cleaned_split = Dataset.from_pandas(df)\n",
    "    print(f\"Original {split_name} size: {initial_size}\")\n",
    "    print(f\"{split_name} size after deduplication: {len(cleaned_split)}\")\n",
    "    return cleaned_split\n",
    "\n",
    "print(\"\\nStarting deduplication process on small samples...\")\n",
    "# Apply deduplication to each split\n",
    "cleaned_dataset['train'] = remove_duplicates_from_split('train', cleaned_dataset['train'])\n",
    "cleaned_dataset['validation'] = remove_duplicates_from_split('validation', cleaned_dataset['validation'])\n",
    "cleaned_dataset['test'] = remove_duplicates_from_split('test', cleaned_dataset['test'])\n",
    "\n",
    "# Verify the structure remains the same\n",
    "print(\"\\nCleaned Dataset Structure (after deduplication):\")\n",
    "print(cleaned_dataset)\n",
    "\n",
    "\n",
    "# --- Step 5: Tokenization ---\n",
    "print(\"\\n--- Step 5: Tokenization ---\")\n",
    "\n",
    "# Choose your pre-trained model checkpoint.\n",
    "# For summarization, common choices are \"facebook/bart-large-cnn\", \"t5-base\", or \"google/pegasus-cnn_dailymail\"\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define maximum input and target lengths.\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"article\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"highlights\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization function to all splits of your dataset.\n",
    "# 'remove_columns' removes the original text columns to save memory, as they are no longer needed.\n",
    "# Now, we only remove the original 'article', 'highlights', 'id' columns.\n",
    "tokenized_dataset = cleaned_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    # num_proc=4, # Keep this commented out or set to 1 for now to avoid multiprocessing errors\n",
    "    # num_proc=1, # Explicitly use a single process for debugging/stability\n",
    "    remove_columns=['article', 'highlights', 'id'] # Removed '__index_level_0__' as it's no longer created\n",
    ")\n",
    "\n",
    "print(\"\\nTokenized Dataset Structure:\")\n",
    "print(tokenized_dataset)\n",
    "print(\"\\nFirst Tokenized Example (training split - showing input_ids and labels):\")\n",
    "print(\"Article Input IDs (first 20):\", tokenized_dataset['train'][0]['input_ids'][:20])\n",
    "print(\"Article Attention Mask (first 20):\", tokenized_dataset['train'][0]['attention_mask'][:20])\n",
    "print(\"Highlights Labels (first 20):\", tokenized_dataset['train'][0]['labels'][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67cf1b53-a8d4-4fc4-842c-8338b67eb459",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;66;03m# For ROUGE metric\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset, DatasetDict \u001b[38;5;66;03m# Ensure Dataset, DatasetDict are imported\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate # For ROUGE metric\n",
    "from datasets import load_dataset, Dataset, DatasetDict # Ensure Dataset, DatasetDict are imported\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM, # For sequence-to-sequence models like BART\n",
    "    DataCollatorForSeq2Seq, # For dynamic padding of batches\n",
    "    Seq2SeqTrainingArguments, # Use Seq2SeqTrainingArguments for seq2seq models\n",
    "    Seq2SeqTrainer, # Use Seq2SeqTrainer for seq2seq models\n",
    "    pipeline\n",
    ")\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb56808-1769-4713-88e3-5fd0c8279b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Load Model, Data Collator, and Metrics ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Move model to GPU if available\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded and moved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Cell 4: Load Model, Define Data Collator, and Metrics\n",
    "print(\"\\n--- Step 6: Load Model, Data Collator, and Metrics ---\")\n",
    "\n",
    "# Load the pre-trained BART model for sequence-to-sequence (Seq2Seq) tasks\n",
    "# 'from_pretrained' will download the weights if not cached.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded and moved to: {device}\")\n",
    "\n",
    "# Data Collator: Dynamically pads input sequences and labels to the longest sequence in the batch.\n",
    "# This is more efficient than static padding to max_length for all examples.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "print(\"Data Collator initialized.\")\n",
    "\n",
    "# Load the ROUGE metric from the 'evaluate' library\n",
    "# ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is standard for summarization.\n",
    "metric = evaluate.load(\"rouge\")\n",
    "print(\"ROUGE metric loaded.\")\n",
    "\n",
    "# Define the compute_metrics function for evaluation during training\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode predictions to text, skipping special tokens\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in labels as we can't decode them.\n",
    "    # -100 is used by the Trainer to ignore padding tokens in loss calculation.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode labels to text, skipping special tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # Add a bit of post-processing to the results (for easier readability)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "print(\"Compute metrics function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e848e82-9530-4c03-a70f-ecf64ad28754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 5: Define Training Arguments and Trainer\n",
    "print(\"\\n--- Step 7: Define Training Arguments and Trainer ---\")\n",
    "\n",
    "# Define Training Arguments for Seq2Seq models\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summarization_results\", # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=3, # Number of training epochs (can increase for full dataset)\n",
    "    per_device_train_batch_size=4, # Reduced for 6GB GPU (effective batch size 4 * 2 = 8 with grad accumulation)\n",
    "    gradient_accumulation_steps=2, # Accumulate gradients over 2 batches\n",
    "    per_device_eval_batch_size=4, # Batch size for evaluation\n",
    "    learning_rate=2e-5, # Learning rate\n",
    "    weight_decay=0.01, # L2 regularization\n",
    "    fp16=torch.cuda.is_available(), # Enable mixed precision if GPU is available\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True, # Load the best model based on evaluation metric\n",
    "    metric_for_best_model=\"eval_loss\", # Use validation loss to determine best model initially\n",
    "    greater_is_better=False, # Lower loss is better\n",
    "    predict_with_generate=True, # Crucial for summarization: generate text during evaluation\n",
    "    logging_dir=\"./summarization_logs\", # Directory for logs\n",
    "    logging_steps=50, # Log training loss every 50 steps\n",
    "    report_to=\"none\", # Prevent external reporting\n",
    "    # Generation parameters for evaluation (can be tuned)\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=4, # Number of beams for beam search decoding\n",
    ")\n",
    "\n",
    "# Initialize the Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics, # Use the ROUGE metric\n",
    ")\n",
    "print(\"Trainer initialized.\")\n",
    "\n",
    "\n",
    "# Cell 6: Execute Training\n",
    "print(\"\\n--- Step 8: Execute Training ---\")\n",
    "print(\"Starting fine-tuning of BART for Summarization...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d930b-e3ef-43d8-a36d-d91442ce3cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (New PyTorch Env)",
   "language": "python",
   "name": "new_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
