{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e9d09b-c6a5-499b-a081-8071bb0e3cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (2025.3.0)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: datasets in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jaggu/anaconda3/envs/new_pytorch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade fsspec datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585f75f7-8c6a-4d26-9569-83fb3f022684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efeac866-2e03-44f1-8b7a-9c58da2d6ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█| 287113/287113 [00:03<00:00, 92089.54 examples/s]\n",
      "Generating validation split: 100%|█| 13368/13368 [00:00<00:00, 112869.63 example\n",
      "Generating test split: 100%|███| 11490/11490 [00:00<00:00, 126943.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n",
      "\n",
      "2. Column Names (Features):\n",
      "['article', 'highlights', 'id']\n",
      "\n",
      "3. First Example from Training Split:\n",
      "Article (first 500 chars):\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s...\n",
      "\n",
      "Highlights:\n",
      "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
      "\n",
      "ID: 42c027e4ff9730fbb3de84c1af0d2c506e41c3e4\n",
      "\n",
      "4. Another Example (e.g., 5th example from validation split):\n",
      "Article (first 500 chars):\n",
      "(CNN)A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online. The parents of Cayman Naib, 13, have been communicating through the Facebook group \"Find Cayman\" since a day after his disappearance, according to close friend David Binswanger. Newtown Police say Cayman was last seen wearing a gray down winter jacket, black ski pants and hiking boots. He could be in the Radn...\n",
      "\n",
      "Highlights:\n",
      "Cayman Naib, 13, hasn't been heard from since Wednesday .\n",
      "Police, family, volunteers search for eighth-grader .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (replace 'cnn_dailymail', '3.0.0' with your chosen dataset if different)\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "print(\"1. Dataset Structure:\")\n",
    "print(dataset) # Shows splits, number of rows, and features\n",
    "\n",
    "print(\"\\n2. Column Names (Features):\")\n",
    "print(dataset['train'].column_names) # Shows the names of columns (e.g., 'article', 'highlights', 'id')\n",
    "\n",
    "print(\"\\n3. First Example from Training Split:\")\n",
    "# Access the first example. Use slicing for long texts to avoid overwhelming output.\n",
    "first_example = dataset['train'][0]\n",
    "print(\"Article (first 500 chars):\")\n",
    "print(first_example['article'][:500] + \"...\")\n",
    "print(\"\\nHighlights:\")\n",
    "print(first_example['highlights'])\n",
    "print(\"\\nID:\", first_example['id'])\n",
    "\n",
    "print(\"\\n4. Another Example (e.g., 5th example from validation split):\")\n",
    "second_example = dataset['validation'][4] # Using index 4 for the 5th example\n",
    "print(\"Article (first 500 chars):\")\n",
    "print(second_example['article'][:500] + \"...\")\n",
    "print(\"\\nHighlights:\")\n",
    "print(second_example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d597c6f6-e6e8-4d47-8853-2f8677f23d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Removing Duplicates ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DatasetDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_split\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Apply to each split\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m cleaned_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetDict\u001b[49m({\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: remove_duplicates_from_split(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m: remove_duplicates_from_split(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: remove_duplicates_from_split(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m })\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Verify the structure remains the same\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCleaned Dataset Structure (after deduplication):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatasetDict' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming 'dataset' is already loaded from Step 1\n",
    "\n",
    "print(\"\\n--- Step 2: Removing Duplicates ---\")\n",
    "\n",
    "# Function to remove duplicates from a single split\n",
    "def remove_duplicates_from_split(split_name, data_split):\n",
    "    initial_size = len(data_split)\n",
    "    df = pd.DataFrame(data_split)\n",
    "    # Define columns that constitute a unique example. For summarization, it's typically article and highlights.\n",
    "    df.drop_duplicates(subset=['article', 'highlights'], inplace=True)\n",
    "    cleaned_split = Dataset.from_pandas(df)\n",
    "    print(f\"Original {split_name} size: {initial_size}\")\n",
    "    print(f\"{split_name} size after deduplication: {len(cleaned_split)}\")\n",
    "    return cleaned_split\n",
    "\n",
    "# Apply to each split\n",
    "cleaned_dataset = DatasetDict({\n",
    "    'train': remove_duplicates_from_split('train', dataset['train']),\n",
    "    'validation': remove_duplicates_from_split('validation', dataset['validation']),\n",
    "    'test': remove_duplicates_from_split('test', dataset['test'])\n",
    "})\n",
    "\n",
    "# Verify the structure remains the same\n",
    "print(\"\\nCleaned Dataset Structure (after deduplication):\")\n",
    "print(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90a05a-febc-4574-a3ea-cb08e2a8ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'cleaned_dataset' is the result from Step 2\n",
    "\n",
    "print(\"\\n--- Step 3: Handling Missing/Empty Values ---\")\n",
    "\n",
    "def remove_empty_examples(example):\n",
    "    # Check if both 'article' and 'highlights' exist and are non-empty strings\n",
    "    return bool(example['article'] and example['highlights'] and\n",
    "                isinstance(example['article'], str) and isinstance(example['highlights'], str))\n",
    "\n",
    "# Apply this filter to all splits\n",
    "cleaned_dataset = cleaned_dataset.filter(remove_empty_examples)\n",
    "\n",
    "print(f\"Train size after removing empty examples: {len(cleaned_dataset['train'])}\")\n",
    "print(f\"Validation size after removing empty examples: {len(cleaned_dataset['validation'])}\")\n",
    "print(f\"Test size after removing empty examples: {len(cleaned_dataset['test'])}\")\n",
    "\n",
    "print(\"\\nCleaned Dataset Structure (after empty examples removal):\")\n",
    "print(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e1c1b-96c7-4e0d-9591-570c3862ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Assuming 'cleaned_dataset' is the result from Step 3\n",
    "\n",
    "print(\"\\n--- Step 4: Basic Text Normalization ---\")\n",
    "\n",
    "def normalize_text_fields(examples):\n",
    "    # Process 'article'\n",
    "    normalized_articles = []\n",
    "    for text in examples['article']:\n",
    "        text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
    "        text = text.strip()              # Remove leading/trailing whitespace\n",
    "        normalized_articles.append(text)\n",
    "\n",
    "    # Process 'highlights'\n",
    "    normalized_highlights = []\n",
    "    for text in examples['highlights']:\n",
    "        text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
    "        text = text.strip()              # Remove leading/trailing whitespace\n",
    "        normalized_highlights.append(text)\n",
    "\n",
    "    return {'article': normalized_articles, 'highlights': normalized_highlights}\n",
    "\n",
    "# Apply this normalization to all splits\n",
    "cleaned_dataset = cleaned_dataset.map(normalize_text_fields, batched=True)\n",
    "\n",
    "print(f\"Example from train split after normalization (first 500 chars):\")\n",
    "print(\"Article:\", cleaned_dataset['train'][0]['article'][:500] + \"...\")\n",
    "print(\"Highlights:\", cleaned_dataset['train'][0]['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21089b09-c609-4a54-9174-b521acd53d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assuming 'cleaned_dataset' is the result from Step 4\n",
    "\n",
    "print(\"\\n--- Step 5: Tokenization ---\")\n",
    "\n",
    "# Choose your pre-trained model checkpoint.\n",
    "# This choice dictates the tokenizer and the model architecture you'll use for fine-tuning.\n",
    "# For summarization, common choices are \"facebook/bart-large-cnn\", \"t5-base\", or \"google/pegasus-cnn_dailymail\"\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define maximum input and target lengths.\n",
    "# These are typically derived from the model's architecture limits and dataset statistics.\n",
    "# For BART/T5, max_input_length is often 1024. For CNN/DM highlights, 128-150 is usually sufficient.\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input articles\n",
    "    # truncation=True: cuts off text longer than max_input_length\n",
    "    # padding=\"max_length\": pads shorter texts to max_input_length with zeros\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"article\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Tokenize the target summaries (highlights)\n",
    "    # The 'labels' key is specifically expected by the Hugging Face Trainer for sequence-to-sequence models.\n",
    "    labels = tokenizer(\n",
    "        examples[\"highlights\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Assign the tokenized labels (input_ids) to the 'labels' key in model_inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization function to all splits of your dataset.\n",
    "# 'remove_columns' removes the original text columns to save memory, as they are no longer needed.\n",
    "tokenized_dataset = cleaned_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4, # Use multiple processes for faster tokenization (adjust based on CPU cores)\n",
    "    remove_columns=cleaned_dataset['train'].column_names # Removes 'article', 'highlights', 'id'\n",
    ")\n",
    "\n",
    "print(\"\\nTokenized Dataset Structure:\")\n",
    "print(tokenized_dataset)\n",
    "print(\"\\nFirst Tokenized Example (training split - showing input_ids and labels):\")\n",
    "print(\"Article Input IDs (first 20):\", tokenized_dataset['train'][0]['input_ids'][:20])\n",
    "print(\"Article Attention Mask (first 20):\", tokenized_dataset['train'][0]['attention_mask'][:20])\n",
    "print(\"Highlights Labels (first 20):\", tokenized_dataset['train'][0]['labels'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3bff85-9e2c-44d7-a4be-ec9521a27867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a00e2-f153-48c7-95b8-44e00bf95ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb9f0f-1a56-4688-9c0e-392cb82b3a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb56808-1769-4713-88e3-5fd0c8279b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e848e82-9530-4c03-a70f-ecf64ad28754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d930b-e3ef-43d8-a36d-d91442ce3cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (New PyTorch Env)",
   "language": "python",
   "name": "new_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
